<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>DABS</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
		<link rel="manifest" href="favicon/site.webmanifest">
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Overview</a></li>
							<li><a href="#one">How it works</a></li>
							<li><a href="#two">Domains</a></li>
							<li><a href="#three">Citation</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">
							<h1>DABS</h1>
							<p class="header">The <strong>D</strong>omain <strong>A</strong>gnostic <strong>B</strong>enchmark for <strong>S</strong>elf-supervised learning.</p>
							<hr class="major">
							<p class="small" >Lorem ipsum dolor sit amet consectetur adipisicing elit. A excepturi et, sequi pariatur magnam dolore eos perspiciatis fugiat necessitatibus aut delectus corporis error iure deleniti non animi perferendis fuga, dolorum soluta, repellat quo odit recusandae aspernatur! Enim nesciunt optio eligendi sunt nobis nulla cupiditate perferendis perspiciatis aspernatur corporis assumenda iste et nostrum voluptates exercitationem impedit consequatur aperiam ab voluptas, facilis ducimus rerum ex reiciendis eaque. Ab voluptas cupiditate sequi distinctio quo illo beatae saepe natus aut nesciunt, possimus eius quos consequatur quasi illum nostrum minima facilis? Totam ea laboriosam expedita, aut modi a, doloremque voluptatem tenetur deleniti nam consequuntur id?</p>
							<ul class="actions">
								<li><a href="#one" class="button scrolly">Learn more</a>
								</li><li><a href="#one" class="button scrolly">Read the Paper</a></li>
								<li><a href="#one" class="button scrolly">See the Code</a></li>
							</ul>
						</div>
					</section>

				<!-- One -->
					<section id="one" class="wrapper style2 spotlights">
						<section>
							<a class="image"><img src="images/Pic01.jpg" alt="" data-position="center center" /></a>
							<div class="content">
								<div class="inner">
									<h2>Embedding</h2>
									<p>First, data from pretraining datasets are embedded into vectors of uniform shape in order to allow for a domain-agnostic model architecture that does not depend on the shape of the data within each domain. We encourage the use of our provided embedding module, but participants may also create their own.</p>
									<ul class="actions">
									</ul>
								</div>
							</div>
						</section>
						<section>
							<a class="image"><img src="images/Pic02.jpg" alt="" data-position="top center" /></a>
							<div class="content">
								<div class="inner">
									<h2>Pretraining</h2>
									<p>Participants have agency over the pretraining objective they choose, and the entire architecture of their model. The goal is to use pretraining datasets to condition a model that is performant accross transfer datasets within a domain, and ultimately to create an architecture and pretraining objective that is performant in this way accross all six domains.</p>
									<ul class="actions">
									
									</ul>
								</div>
							</div>
						</section>
						<section>
							<a class="image"><img src="images/Pic03.jpg" alt="" data-position="25% 25%" /></a>
							<div class="content">
								<div class="inner">
									<h2>Transfer Learning</h2>
									<p>In the adaptation layer, the model is given labeled data in the same domain as the pretraining data, but possibly from a different dataset and with a different end task. A linear classifier is provided as the adaptation layer in our baseline model, but participants may choose their own adaptation method so long as it is in the spirit of the benchmark.</p>
									<ul class="actions">
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									</ul>
								</div>
							</div>
						</section>
					</section>

				<!-- Two -->
					<section id="two" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>Domains</h2>
							<p>We use six domains in order to capture performance over both traditional ML tasks that have extensive research communities (e.g. computer vision and NLP) and less studied/emerging focal point of the field (e.g. sensor and x-ray data). More information about selection criteria and specific datasets can be found in the DABS paper.</p>
							<p>Select a domain to view more information about the datasets used.</p>
							<div class="features">
								<section class="domain" data-link="domains/images.html">
									<span class="icon solid major fa-image"></span>
									<h3>Natural Images</h3>
									<p>Pretraining is done on the ImageNet dataset, and then transfer tasks cover a wide variety of natural image classification tasks.</p>
								</section>
								<section>
									<span class="icon solid major fa-comments"></span>
									<h3>Speech Recordings</h3>
									<p>Pretraining is done on the Librispeech corpus, a large dataset of English speech recordings. Downstream tasks cover everything from speaker recognition to utterance classification.</p>
								</section>
								<section class="domain" data-link="domains/text.html">
										<span class="icon solid major fa-font"></span>
										<h3>Text</h3>
										<p>In order to encourage the production of models that function accross cultural lines, we use the multilingual mC4 dataset for pretraining in the text domain, and the PAWS-X multilingual paraphrase detection tasks for transfer tasks.</p>
								</section>
								<section>
									<span class="icon solid major fa-running"></span>
									<h3>Sensor</h3>
									<p>Pretraining and transfer tasks are both derived from the PAMAP-2 dataset, a dataset consisting of readings from many body sensors over time, and labels describing the task the monitered people were doing at the time of measurement.</p>
								</section>
								<section>
									<span class="icon solid major fa-x-ray"></span>
									<h3>Chest X-ray</h3>
									<p> For both pretraining and transfer tasks, we use the CheXpert dataset, which contain chest X-rays labeled with the particular conditions identafiable in each by human pathologists.</p>
								</section>
								<section>
									<span class="icon major fa-file-image"></span>
									<h3>Images with paired Text captions</h3>
									<p>To attain coverage of multimodal domains, we include captioned images. Pretraining is done on the COCO dataset, with downstream transfer performance assessed by the Visual Question Answering task.</p>
								</section>
								<section>
									<span class="icon solid major fa-language"></span>
									<h3>Multi-language Text</h3>
									<p>To attain coverage of multimodal domains, we include captioned images. Pretraining is done on the COCO dataset, with downstream transfer performance assessed by the Visual Question Answering task.</p>
								</section>
								<section>
									<span class="icon solid major fa-atom"></span>
									<h3>Physics</h3>
									<p>To attain coverage of multimodal domains, we include captioned images. Pretraining is done on the COCO dataset, with downstream transfer performance assessed by the Visual Question Answering task.</p>
								</section>
							</div>
							<p>For information about dataset usage, and data origins, please see the appendix of the <a>DABS paper.</a></p>
						</div>
					</section>
					<script type="text/javascript">
						$(document).ready(function() {
							$(".domain").click(function() {
								window.location = window.location.href.slice(0, window.location.href.indexOf("index.html")) + $(this).data('link')
							});
						});
					</script>

				<!-- Three -->
					<section id="three" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Using DABS?</h2>
							<p>We appreciate your desire to contribute or build on DABS! We only ask that you cite our paper when doing so. You may use the bibtex snippet below to cite our work.</p> 
							<br>
							<pre><code>
@techreport{WahCUB_200_2011,
	Title = {{The Caltech-UCSD Birds-200-2011 Dataset}},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011}
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}
						</code></pre>
							<br>
							<br>
						</div>
					</section>


			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper style1-alt">
				<div class="inner">
					<ul class="menu">
						<li>DABS is released under MIT License</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>