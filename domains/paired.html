<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>DABS</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<link rel="apple-touch-icon" sizes="180x180" href="../favicon/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../favicon/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../favicon/favicon-16x16.png">
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../index.html" class="title">DABS</a>
				<nav>
					<ul>
						<li><a href="../index.html">Home</a></li>
						<li><a href="../team.html">Meet the Team</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<span class="image fit"><img src="../images/domains/paired.png" alt="" /></span>
							<h1 class="major">Paired Image and Text</h1>
							<h2>Pretraining</h2> 
							<div class="dataset">
								<h4>MSCOCO</h4>
								<p>The <a href=”https://cocodataset.org/” >MSCOCO</a> (Microsoft Common Objects in Context) dataset consists of over 300K images and captions. The dataset contains a wide range of natural images, as well as accompanying natural language descriptions of those images.</p>
							</div>
							<h2>Transfer</h2>
							<div class="dataset">
								<h4>MSCOCO-Matching</h4>
								<p>The MSCOCO-Matching task uses the <a href="https://cocodataset.org/" >MSCOCO</a> dataset. The image for a given image-caption pair is replaced for 50% of the examples and the task is to predict whether the image and caption are correctly matched.</p>
							</div>
							<div class="dataset">
								<h4>VQA</h4>
								<p>The <a href="https://visualqa.org/download.html" >VQA</a> dataset consists of over 200K images along with open-ended questions and answers. Commonsense knowledge as well as an understanding of vision and language are required to answer the questions. The images are derived from the Common Objects in Context (COCO) dataset. We convert VQA into a binary classification task where the task is to predict whether the given question and answer are aligned with the provided image. Incorrect answers are chosen from a provided set of plausible answers.</p>
							</div>
							<a href="../citations/paired.html"  style="margin-left: 20px;" class="button" >Cite Datasets</a>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>DABS is released under MIT License.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>