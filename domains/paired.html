<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>DABS</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<link rel="apple-touch-icon" sizes="180x180" href="../favicon/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../favicon/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../favicon/favicon-16x16.png">
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../index.html" class="title">DABS</a>
				<nav>
					<ul>
						<li><a href="../index.html">Home</a></li>
						<li><a href="../team.html">Meet the Team</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Paired Image and Text</h1>
							<p>Multi-modal tasks represent a rapidly growing and useful area of machine learning. By including the captioned image task, we hope to encourage the development of models that can perform well accross data structures.</p>
							<span class="image fit"><img src="../images/domains/lorem_ipsum.png" alt="" /></span>
							<h2>Pretraining</h2> 
							<div class="dataset">
								<h4>MSCOCO</h4>
								<p>The <a href="https://cocodataset.org/" >MSCOCO</a> (Microsoft Common Objects in Context) dataset consists of over 300K images and captions. Images are split into training, validation, and test sets. The dataset has annotations for object detection (bounding boxes and per-instance segmentation masks with 80 object categories), captioning (natural language descriptions of the images), keypoints detection (17 possible keypoints, such as left eye, nose), stuff image segmentation (per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky), panoptic (full scene segmentation, with 80 thing categories and a subset of 91 stuff categories),  and DensePose annotations. Training and validation images have publicly available annotations. Over 100K unannotated images are also included. </p>
							</div>
							<h2>Transfer</h2>
							<div class="dataset">
								<h4>VQA</h4>
								<p>The <a href="https://visualqa.org/download.html" >VQA</a> dataset consists of over 200K images along with open-ended questions and answers. Commonsense knowledge as well as an understanding of vision and language are required to answer the questions. The images are of Common Objects in Context (COCO) and abstract scenes. The database contains an average of 5.4 questions per image (minimum of 3). For each question, 10 ground truth answers and 3 plausible (but likely incorrect) answers are included. The database also has an automatic evaluation metric.</p>
							</div>
							<a href="../citations/paired.html"  style="margin-left: 20px;" class="button" >Cite Datasets</a>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>DABS is released under MIT License.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>